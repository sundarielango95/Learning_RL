{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcNZPZWyOEtc"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n\n",
        "row = 5\n",
        "col = 5\n",
        "\n",
        "# Actions\n",
        "SOUTH = 0\n",
        "NORTH = 1\n",
        "EAST = 2\n",
        "WEST = 3\n",
        "PICK = 4\n",
        "DROP = 5\n",
        "total_actions = [SOUTH, NORTH, EAST, WEST, PICK, DROP]\n",
        "primitive_actions= [SOUTH, NORTH, EAST, WEST]\n",
        "num_pacts = len(primitive_actions)\n",
        "\n",
        "# Primary Destinations for Pickup and Drop\n",
        "RED = 0\n",
        "GREEN = 1\n",
        "YELLOW = 2\n",
        "BLUE = 3\n",
        "IN_TAXI = 4\n",
        "\n",
        "passenger_locs = [RED, GREEN, YELLOW, BLUE, IN_TAXI]\n",
        "destinations = [RED, GREEN, YELLOW, BLUE]\n",
        "destination_coords = [[0,0], [0,4], [4,0], [4,3]]\n",
        "\n",
        "num_plocs = len(passenger_locs)\n",
        "num_dlocs = len(destinations)\n",
        "\n",
        "# cur_state = ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination"
      ],
      "metadata": {
        "id": "YOO_KG8wOK8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-greedy policy\n",
        "def choose_option_action_eg(qvals, num_choices, eps):\n",
        "    '''\n",
        "    Choosing the option/action for the current state\n",
        "    based on an exploration-exploitation trade-off\n",
        "    using epsilon-greedy policy\n",
        "    '''\n",
        "    if np.random.rand() < eps:\n",
        "        # Exploration\n",
        "        option = np.random.choice(num_choices)\n",
        "\n",
        "    else:\n",
        "        # Exploitation\n",
        "        option = np.argmax(qvals)\n",
        "\n",
        "    return option\n",
        "\n",
        "def choose_action_eg(qvals, num_pacts, eps):\n",
        "    '''\n",
        "    Choosing the action for the current state\n",
        "    based on an exploration-exploitation trade-off\n",
        "    using epsilon-greedy policy\n",
        "    '''\n",
        "    if np.random.rand() < eps:\n",
        "        # Exploration\n",
        "        action = np.random.choice(num_pacts)\n",
        "\n",
        "    else:\n",
        "        # Exploitation\n",
        "        action = np.argmax(qvals)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "3Edbuw-ZOgf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_passenger_in_taxi(ploc):\n",
        "    '''\n",
        "    Checking if passenger in taxi\n",
        "    '''\n",
        "    if ploc == 4:\n",
        "        ptaxi = True\n",
        "    else:\n",
        "        ptaxi = False\n",
        "    return ptaxi"
      ],
      "metadata": {
        "id": "vY6Cvzz-OxsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Options\n",
        "\n",
        "class Option:\n",
        "    def __init__(self, desgn_loc, corner):\n",
        "        self.goal_loc = desgn_loc\n",
        "        self.corner = corner # Side of the environment denoting option/goal location\n",
        "        self.qvals = np.zeros((row, col, num_pacts))\n",
        "        self.sa_freq = np.zeros((row, col, num_pacts))\n",
        "\n",
        "    def reset(self):\n",
        "        self.qvals = np.zeros((row, col, num_pacts))\n",
        "        self.sa_freq = np.zeros((row, col, num_pacts))\n",
        "\n",
        "    def execute(self, state, dloc, eps):\n",
        "        '''\n",
        "        Choosing primitive action within the option\n",
        "        and determining option termination\n",
        "\n",
        "        '''\n",
        "        dstate = destination_coords[dloc]\n",
        "        optdone = False\n",
        "\n",
        "        if state[0] == self.goal_loc[0] and state[1] == self.goal_loc[1]:\n",
        "            optdone = True\n",
        "            if self.corner == 0:\n",
        "                # Top\n",
        "                if dloc == GREEN:\n",
        "                    optact = EAST\n",
        "                else:\n",
        "                    optact = SOUTH\n",
        "\n",
        "            if self.corner == 1:\n",
        "                # Right\n",
        "                if dloc == GREEN:\n",
        "                    optact = NORTH\n",
        "                elif dloc == BLUE:\n",
        "                    optact = SOUTH\n",
        "                else:\n",
        "                    optact = WEST\n",
        "\n",
        "            if self.corner == 2:\n",
        "                # Left\n",
        "                if dloc == RED:\n",
        "                    optact = NORTH\n",
        "                elif dloc == YELLOW:\n",
        "                    optact = SOUTH\n",
        "                else:\n",
        "                    optact = EAST\n",
        "\n",
        "            if self.corner == 3:\n",
        "                # Middle\n",
        "                if dloc == GREEN or dloc == BLUE:\n",
        "                    optact = EAST\n",
        "                if dloc == RED or dloc == YELLOW:\n",
        "                    optact = WEST\n",
        "        else:\n",
        "            optact = choose_action_eg(self.qvals[state[0], state[1]], num_pacts, eps)\n",
        "\n",
        "        return optact, optdone"
      ],
      "metadata": {
        "id": "oE31UqbZocZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning parameters\n",
        "num_episodes = 5000\n",
        "num_runs = 5\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Epsilon greedy\n",
        "eps_decay = 0.99\n",
        "eps_start = 0.5\n",
        "eps_final = 0.01"
      ],
      "metadata": {
        "id": "5ruQYL3mRVFi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training agent\n",
        "\n",
        "run_returns = []\n",
        "run_qo = []\n",
        "run_options = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f'Beginning Run {run+1}')\n",
        "\n",
        "    # Creating Options\n",
        "    Top = Option([0,2], 0)\n",
        "    Right = Option([2,4], 1)\n",
        "    Left = Option([2,0], 2)\n",
        "    Middle = Option([2,2], 3)\n",
        "\n",
        "    options = [Top, Right, Left, Middle]\n",
        "    option_names = ['Top', 'Right', 'Left', 'Middle']\n",
        "    num_options = len(options)\n",
        "\n",
        "    total_choices = [SOUTH, NORTH, EAST, WEST, Top, Right, Left, Middle, PICK, DROP]\n",
        "\n",
        "    q_o = np.zeros((num_plocs*num_dlocs, len(total_choices)))\n",
        "\n",
        "    eps = eps_start\n",
        "\n",
        "    ep_rew = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        # print(f'\\n Episode {ep} begins!\\n')\n",
        "        episode_reward = 0\n",
        "\n",
        "        # Initial state\n",
        "        state = env.reset()\n",
        "        tx, ty, ploc, dloc = list(env.decode(state))\n",
        "        t_coord = [tx, ty]\n",
        "\n",
        "        # Checking if passenger is in taxi\n",
        "        ptaxi = check_passenger_in_taxi(ploc)\n",
        "\n",
        "        done = False # Episode flag\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Get option/action using ep-greedy\n",
        "            opt_state = num_dlocs*ploc + dloc\n",
        "            opt_act_id = choose_option_action_eg(q_o[opt_state], len(total_choices), eps)\n",
        "\n",
        "            if opt_act_id == 8:\n",
        "              # When next action chosen is to pick up the passenger\n",
        "                ptaxi = check_passenger_in_taxi(ploc)\n",
        "\n",
        "                if not ptaxi:\n",
        "                    pstate = destination_coords[ploc]\n",
        "                    if pstate[0] == tx and pstate[1] == ty:\n",
        "                        # Pick up passenger\n",
        "                        action = total_choices[opt_act_id]\n",
        "                        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "                        tnx, tny, pnloc, dnloc = list(env.decode(next_state))\n",
        "                        tn_coord = [tnx, tny]\n",
        "                        opt_ns = num_dlocs*pnloc + dnloc\n",
        "\n",
        "                        episode_reward += reward\n",
        "\n",
        "                        # Update option-action\n",
        "                        q_o[opt_state, opt_id] = q_o[opt_state, opt_id] + alpha * (reward + gamma * ((1-done)*q_o[opt_ns, opt_id] +\n",
        "                                                                                                  done*np.max(q_o[opt_ns,:])) - q_o[opt_state, opt_id])\n",
        "\n",
        "                        state = next_state\n",
        "                        tx, ty, ploc, dloc = list(env.decode(state))\n",
        "                        t_coord = [tx, ty]\n",
        "                        opt_state = opt_ns\n",
        "\n",
        "            elif opt_act_id == 9:\n",
        "                # When next action chosen is to drop passenger\n",
        "                ptaxi = check_passenger_in_taxi(ploc)\n",
        "                dstate = destination_coords[dloc]\n",
        "\n",
        "                if ptaxi:\n",
        "                    if dstate[0] == tx and dstate[1] == ty:\n",
        "                        # Drop passenger\n",
        "                        action = total_choices[opt_act_id]\n",
        "                        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "                        tnx, tny, pnloc, dnloc = list(env.decode(next_state))\n",
        "                        tn_coord = [tnx, tny]\n",
        "                        opt_ns = num_dlocs*pnloc + dnloc\n",
        "\n",
        "                        episode_reward += reward\n",
        "\n",
        "                        # Update option-action\n",
        "                        q_o[opt_state, opt_id] = q_o[opt_state, opt_id] + alpha * (reward + gamma * ((1-done)*q_o[opt_ns, opt_id] +\n",
        "                                                                                                  done*np.max(q_o[opt_ns,:])) - q_o[opt_state, opt_id])\n",
        "\n",
        "                        state = next_state\n",
        "                        tx, ty, ploc, dloc = list(env.decode(state))\n",
        "                        t_coord = [tx, ty]\n",
        "                        opt_state = opt_ns\n",
        "\n",
        "\n",
        "            elif opt_act_id < 4:\n",
        "                # When next action chosen is a primitive action\n",
        "                opt_id = opt_act_id\n",
        "\n",
        "                # Get next state, reward, done(episode), info(term prob, action mask)\n",
        "                action = total_choices[opt_id]\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "\n",
        "                tnx, tny, pnloc, dnloc = list(env.decode(next_state))\n",
        "                tn_coord = [tnx, tny]\n",
        "                opt_ns = num_dlocs*pnloc + dnloc\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Update option-action\n",
        "                q_o[opt_state, opt_id] = q_o[opt_state, opt_id] + alpha * (reward + gamma * ((1-done)*q_o[opt_ns, opt_id] +\n",
        "                                                                                          done*np.max(q_o[opt_ns,:])) - q_o[opt_state, opt_id])\n",
        "\n",
        "\n",
        "                state = next_state\n",
        "                tx, ty, ploc, dloc = list(env.decode(state))\n",
        "                t_coord = [tx, ty]\n",
        "                opt_state = opt_ns\n",
        "\n",
        "            else:\n",
        "                # When next action chosen is an option\n",
        "                opt_id = opt_act_id\n",
        "                opt = total_choices[opt_id]\n",
        "                optdone = False # Option flag\n",
        "\n",
        "                # Execute option till termination\n",
        "                while not optdone:\n",
        "                    ptaxi = check_passenger_in_taxi(ploc)\n",
        "\n",
        "                    # Get primitive action, optact and optdone\n",
        "                    optact, optdone = opt.execute([tx, ty], dloc, eps)\n",
        "\n",
        "                    # Get next state, reward, done(episode), info(term prob, action mask)\n",
        "                    next_state, reward, done, info = env.step(optact)\n",
        "\n",
        "                    tnx, tny, pnloc, dnloc = list(env.decode(next_state))\n",
        "                    tn_coord = [tnx, tny]\n",
        "                    opt_ns = num_dlocs*pnloc + dnloc\n",
        "                    # print([tx,ty], [tnx,tny], reward, optdone, done, ploc, dloc, pnloc, dnloc)\n",
        "\n",
        "                    episode_reward += reward\n",
        "\n",
        "                    # Update primitive actions\n",
        "                    if optact < 4:\n",
        "                        opt.qvals[tx, ty, optact] = opt.qvals[tx, ty, optact] + alpha * (reward + gamma*np.max(opt.qvals[tnx, tny]) - opt.qvals[tx, ty, optact])\n",
        "                        opt.sa_freq[tx, ty, optact] += 1\n",
        "                        if optdone:\n",
        "                            opt.sa_freq[tnx, tny, optact] += 1\n",
        "\n",
        "                    # Update option\n",
        "                    q_o[opt_state, opt_id] = q_o[opt_state, opt_id] + alpha * (reward + gamma * ((1-optdone)*q_o[opt_ns, opt_id] +\n",
        "                                                                                          optdone*np.max(q_o[opt_ns,:])) - q_o[opt_state, opt_id])\n",
        "\n",
        "                    # Updating consistent options\n",
        "                    for op in range(4, 4+num_options):\n",
        "                        if opt_id != op:\n",
        "                            other_opt = total_choices[op]\n",
        "                            optact_, optdone_ = other_opt.execute([tx, ty], dloc, eps)\n",
        "                            if optact_ == optact:\n",
        "                                q_o[opt_state, op] = q_o[opt_state, op] + alpha * (reward + gamma * ((1-optdone_)*q_o[opt_ns, op] +\n",
        "                                                                                              optdone_*np.max(q_o[opt_ns,:])) - q_o[opt_state, op])\n",
        "\n",
        "                    state = next_state\n",
        "                    tx, ty, ploc, dloc = list(env.decode(state))\n",
        "                    t_coord = [tx, ty]\n",
        "                    opt_state = opt_ns\n",
        "\n",
        "\n",
        "        eps = max(eps_final, eps_decay*eps)\n",
        "        ep_rew.append(episode_reward)\n",
        "        ep_rew_avg100 = [np.average(ep_rew[i:i+100]) for i in range(len(ep_rew)-100)]\n",
        "\n",
        "    print(f'Run: {run+1} - Max reward: {max(ep_rew)}')\n",
        "    run_returns.append(ep_rew)\n",
        "    run_qo.append(q_o)\n",
        "    run_options.append([Top, Right, Left, Middle])"
      ],
      "metadata": {
        "id": "ZG-_9oI7RXEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs\n",
        "\n",
        "returns_mean = np.mean(run_returns, 0)\n",
        "plt.plot(returns_mean)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns')\n",
        "plt.title('IntraOption Q-Learning (Episodic Returns averaged over 5 runs)')"
      ],
      "metadata": {
        "id": "XwCrfwuedwbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Maximum return over 5 runs {max(returns_mean)}')"
      ],
      "metadata": {
        "id": "0L5xTZKzNBgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - 400th-5000th episode to visualize exact reward values\n",
        "xvals = range(400, 5000)\n",
        "plt.plot(xvals, returns_mean[400:5000])\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns')\n",
        "plt.title(f'IntraOption Q-Learning (Episodic Returns averaged over 5 runs)\\n400th-5000th episode to visualize exact reward values')"
      ],
      "metadata": {
        "id": "aUTumV6GNBgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging returns every 100 episodes (over 5 runs)\n",
        "return_avg100 = [np.average(returns_mean[i:i+100]) for i in range(len(returns_mean)-100)]"
      ],
      "metadata": {
        "id": "PYGaecmyNBgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Maximum return over 5 runs (returns averaged every 100 episodes in each run) {max(return_avg100)}')"
      ],
      "metadata": {
        "id": "wvZGqUERNBgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - returns in every run averaged every 100 episodes\n",
        "plt.plot(return_avg100)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns (averaged every 100 episodes)')\n",
        "plt.title('IntraOption Q-Learning (Episodic Returns averaged over 5 runs)')"
      ],
      "metadata": {
        "id": "AbtqfnXtNBgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - returns in every run averaged every 100 episodes\n",
        "# From 400th episode to visualize exact reward values\n",
        "xvals = range(400,4900)\n",
        "plt.plot(xvals, return_avg100[400:4900])\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns (averaged every 100 episodes)')\n",
        "plt.title(f'IntraOption Q-Learning (Episodic Returns averaged over 5 runs)\\nFrom 400th episode to visualize exact reward values')"
      ],
      "metadata": {
        "id": "8khAi-uYNBgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Q values on heatmap\n",
        "\n",
        "def plotQvals(Q, message = \"Q plot\"):\n",
        "    # Visualize Q values across taxi env\n",
        "    Q = np.flip(Q, 0)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(message)\n",
        "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
        "    plt.colorbar()\n",
        "    def x_direct(a):\n",
        "        if a in [NORTH, SOUTH]:\n",
        "            return 0\n",
        "        return 1 if a == EAST else -1\n",
        "    def y_direct(a):\n",
        "        if a in [EAST, WEST]:\n",
        "            return 0\n",
        "        return 1 if a == NORTH else -1\n",
        "    policy = Q.argmax(-1)\n",
        "    policyx = np.vectorize(x_direct)(policy)\n",
        "    policyy = np.vectorize(y_direct)(policy)\n",
        "    idx = np.indices(policy.shape)\n",
        "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MVw7O3r9NBgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Q values of option policies - Top, Right, Left, Middle\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
        "# fig.title('Q value visualization for the Option policies')\n",
        "\n",
        "f1 = plotQvals(Top.qvals, 'Learnt Policy of Option Top')\n",
        "f2 = plotQvals(Right.qvals, 'Learnt Policy of Option Right')\n",
        "f3 = plotQvals(Left.qvals, 'Learnt Policy of Option Left')\n",
        "f4 = plotQvals(Middle.qvals, 'Learnt Policy of Option Middle')"
      ],
      "metadata": {
        "id": "CWOWGypjNBgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize state-action frequencies on heatmap\n",
        "\n",
        "def plot_state_visit(state_visits, message = 'Mean Number of State Visits over 5 runs (5000 episodes each)'):\n",
        "    # Visualize number of times agent visits each state\n",
        "    state_visits = np.flip(state_visits, 0)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(message)\n",
        "    plt.pcolor(state_visits.max(-1), edgecolors='k', linewidths=2, cmap=\"GnBu\")\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2F1FhLx2094L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing state-action frequencies of option policies - Top, Right, Left, Middle\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
        "# fig.title('Q value visualization for the Option policies')\n",
        "\n",
        "f1 = plot_state_visit(Top.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Top')\n",
        "f2 = plot_state_visit(Right.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Right')\n",
        "f3 = plot_state_visit(Left.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Left')\n",
        "f4 = plot_state_visit(Middle.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Middle')"
      ],
      "metadata": {
        "id": "p2ah_-iD9Zt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}