{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww1ES48zJBsF"
      },
      "source": [
        "# CS6700: Tutorial 3 - Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xEl5AbPCamd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9tjMH9cgH29"
      },
      "source": [
        "Consider a standard grid world, where only 4 (up, down, left, right) actions are allowed and the agent deterministically moves accordingly, represented as below. Here yellow is the start state and white is the goal state.\n",
        "\n",
        "Say, we define our MDP as:\n",
        "- S: 121 (11 x 11) cells\n",
        "- A: 4 actions (up, down, left, right)\n",
        "- P: Deterministic transition probability\n",
        "- R: -1 at every step\n",
        "- gamma: 0.9\n",
        "\n",
        "Our goal is to find an optimal policy (shown in right).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pLSfisLKgKsT"
      },
      "outputs": [],
      "source": [
        "# Above grid is defined as below:\n",
        "#   - 0 denotes an navigable tile\n",
        "#   - 1 denotes an obstruction/wall\n",
        "#   - 2 denotes the start state\n",
        "#   - 3 denotes an goal state\n",
        "\n",
        "# Note: Here the upper left corner is defined as (0, 0)\n",
        "#       and lower right corner as (m-1, n-1)\n",
        "\n",
        "# Optimal Path: RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n",
        "\n",
        "\n",
        "GRID_WORLD = np.array([\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
        "    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTHAOARUWoC"
      },
      "source": [
        "### Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FZ3Eh_bRkBFt"
      },
      "outputs": [],
      "source": [
        "class Actions(Enum):\n",
        "  UP    = (0, (-1, 0))  # index = 0, (xaxis_move = -1 and yaxis_move = 0)\n",
        "  DOWN  = (1, (1, 0))   # index = 1, (xaxis_move = 1 and yaxis_move = 0)\n",
        "  LEFT  = (2, (0, -1))  # index = 2, (xaxis_move = 0 and yaxis_move = -1)\n",
        "  RIGHT = (3, (0, 1))   # index = 3, (xaxis_move = 0 and yaxis_move = -1)\n",
        "\n",
        "  def get_action_dir(self):\n",
        "    _, direction = self.value\n",
        "    return direction\n",
        "\n",
        "  @property\n",
        "  def index(self):\n",
        "    indx, _ = self.value\n",
        "    return indx\n",
        "\n",
        "  @classmethod\n",
        "  def from_index(cls, index):\n",
        "    action_index_map = {a.index: a for a in cls}\n",
        "    return action_index_map[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDSO8vMfkkdB",
        "outputId": "da5e2602-bc96-4c91-e495-7340d0692a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: UP, action_id: 0, direction_to_move: (-1, 0)\n",
            "name: DOWN, action_id: 1, direction_to_move: (1, 0)\n",
            "name: LEFT, action_id: 2, direction_to_move: (0, -1)\n",
            "name: RIGHT, action_id: 3, direction_to_move: (0, 1)\n",
            "\n",
            "------------------------------------\n",
            "\n",
            "0 index action is: UP\n"
          ]
        }
      ],
      "source": [
        "# How to use Action enum\n",
        "for a in Actions:\n",
        "  print(f\"name: {a.name}, action_id: {a.index}, direction_to_move: {a.get_action_dir()}\")\n",
        "\n",
        "print(\"\\n------------------------------------\\n\")\n",
        "\n",
        "# find action enum from index 0\n",
        "a = Actions.from_index(0)\n",
        "print(f\"0 index action is: {a.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b53SkgJlDIt"
      },
      "source": [
        "### Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u5SyrH9vkepn"
      },
      "outputs": [],
      "source": [
        "class BasePolicy:\n",
        "  def update(self, *args):\n",
        "    pass\n",
        "\n",
        "  def select_action(self, state_id: int) -> int:\n",
        "    raise NotImplemented\n",
        "\n",
        "\n",
        "class DeterministicPolicy(BasePolicy):\n",
        "  def __init__(self, actions: np.ndarray):\n",
        "    # actions: its a 1d array (|S| size) which contains action for each state\n",
        "    self.actions = actions\n",
        "\n",
        "  def update(self, state_id, action_id):\n",
        "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
        "    assert action_id < len(Actions), f\"Invalid action_id {action_id}\"\n",
        "    self.actions[state_id] = action_id\n",
        "\n",
        "  def select_action(self, state_id: int) -> int:\n",
        "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
        "    return self.actions[state_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4I7x4rMlFgp"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zt-gPaPOldH0"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "  def __init__(self, grid):\n",
        "    self.grid = grid\n",
        "    m, n = grid.shape\n",
        "    self.num_states = m*n\n",
        "\n",
        "  def xy_to_posid(self, x: int, y: int):\n",
        "    _, n = self.grid.shape\n",
        "    return x*n + y\n",
        "\n",
        "  def posid_to_xy(self, posid: int):\n",
        "    _, n = self.grid.shape\n",
        "    return (posid // n, posid % n)\n",
        "\n",
        "  def isvalid_move(self, x: int, y: int):\n",
        "    m, n = self.grid.shape\n",
        "    return (x >= 0) and (y >= 0) and (x < m) and (y < n) and (self.grid[x, y] != 1)\n",
        "\n",
        "  def find_start_xy(self) -> int:\n",
        "    m, n = self.grid.shape\n",
        "    for x in range(m):\n",
        "      for y in range(n):\n",
        "        if self.grid[x, y] == 2:\n",
        "          return (x, y)\n",
        "    raise Exception(\"Start position not found.\")\n",
        "\n",
        "  def find_path(self, policy: BasePolicy) -> str:\n",
        "    max_steps = 50\n",
        "    steps = 0\n",
        "\n",
        "    P, R = self.get_transition_prob_and_expected_reward()\n",
        "    num_actions, num_states = R.shape\n",
        "    all_possible_state_posids = np.arange(num_states)\n",
        "\n",
        "    path = \"\"\n",
        "    curr_x, curr_y = self.find_start_xy()\n",
        "    while (self.grid[curr_x, curr_y] != 3) and (steps < max_steps):\n",
        "      curr_posid = self.xy_to_posid(curr_x, curr_y)\n",
        "      action_id = policy.select_action(curr_posid)\n",
        "      next_posid = np.random.choice(\n",
        "          all_possible_state_posids, p=P[action_id, curr_posid])\n",
        "      action = Actions.from_index(action_id)\n",
        "      path += f\" {action.name}\"\n",
        "      curr_x, curr_y = self.posid_to_xy(next_posid)\n",
        "      steps += 1\n",
        "    return path\n",
        "\n",
        "  def get_transition_prob_and_expected_reward(self):  # P(s_next | s, a), R(s, a)\n",
        "    m, n = self.grid.shape\n",
        "    num_states = m*n\n",
        "    num_actions = len(Actions)\n",
        "    P = np.zeros((num_actions, num_states, num_states))\n",
        "    R = np.zeros((num_actions, num_states))\n",
        "    for a in Actions:\n",
        "      for x in range(m):\n",
        "        for y in range(n):\n",
        "          xmove_dir, ymove_dir = a.get_action_dir()\n",
        "          xnew, ynew = x + xmove_dir, y + ymove_dir  # find the new co-ordinate after the action a\n",
        "\n",
        "          posid = self.xy_to_posid(x, y)\n",
        "          new_posid = self.xy_to_posid(xnew, ynew)\n",
        "\n",
        "\n",
        "          if self.grid[x, y] == 3:\n",
        "            # the current state is a goal state\n",
        "            P[a.index, posid, posid] = 1\n",
        "            R[a.index, posid] = 0\n",
        "          elif (self.grid[x, y] == 1) or (not self.isvalid_move(xnew, ynew)):\n",
        "            # the current state is a block state or the next state is invalid\n",
        "            P[a.index, posid, posid] = 1\n",
        "            R[a.index, posid] = -1\n",
        "          else:\n",
        "            # action a is valid and goes to a new position\n",
        "            P[a.index, posid, new_posid] = 1\n",
        "            R[a.index, posid] = -1\n",
        "    return P, R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXLPFU_DUJMw"
      },
      "source": [
        "### Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BuPTRrJBsN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vi5e3FAHKoZY"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(P: np.ndarray, R: np.ndarray, gamma: float,\n",
        "                      policy: BasePolicy, theta: float,\n",
        "                      init_V: np.ndarray=None):\n",
        "  _, num_states = R.shape\n",
        "\n",
        "  # Please try different starting point for V you will find it will always\n",
        "  # converge to the same V_pi value.\n",
        "\n",
        "  if init_V is None:\n",
        "    init_V = np.zeros(num_states)\n",
        "\n",
        "  V = copy.deepcopy(init_V)\n",
        "\n",
        "  delta = 100.0\n",
        "\n",
        "  while delta > theta:\n",
        "    delta = 0.0\n",
        "\n",
        "    for state_id in range(num_states):\n",
        "\n",
        "      action_id = policy.select_action(state_id)\n",
        "\n",
        "      # Following equation is a different way of writing the same equation given in the slide.\n",
        "      # Note here R is an expected reward term.\n",
        "      oldV = V[state_id]\n",
        "      V[state_id] = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      delta = max(delta,np.abs(oldV-V[state_id])) # Calculate delta which determines when to terminate the evaluation step\n",
        "\n",
        "  return V\n",
        "\n",
        "\n",
        "def policy_improvement(P: np.ndarray, R: np.ndarray, gamma: float,\n",
        "                      policy: BasePolicy, V: np.ndarray):\n",
        "  _, num_states = R.shape\n",
        "  policy_stable = True\n",
        "\n",
        "  for state_id in range(num_states):\n",
        "\n",
        "    old_action_id = policy.select_action(state_id)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    VAction = np.zeros(len(Actions))\n",
        "    for action_id in range(len(Actions)):\n",
        "      VAction[action_id] = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)\n",
        "    new_action_id = np.argmax(VAction) # update new_action_id based on the value function.\n",
        "\n",
        "    policy.update(state_id, new_action_id)\n",
        "    if old_action_id != new_action_id:\n",
        "      policy_stable = False\n",
        "\n",
        "  return policy_stable\n",
        "\n",
        "\n",
        "def policy_iteration(P: np.ndarray, R: np.ndarray, gamma: float,\n",
        "                    theta: float=1e-3, init_policy: BasePolicy = None):\n",
        "  _, num_states = R.shape\n",
        "\n",
        "  # Please try exploring different policies you will find it will always\n",
        "  # converge to the same optimal policy for valid states.\n",
        "  if init_policy is None:\n",
        "    # Say initial policy = all up actions.\n",
        "    init_policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))\n",
        "\n",
        "  # creating a copy of a initial policy\n",
        "  policy = copy.deepcopy(init_policy)\n",
        "  policy_stable = False\n",
        "\n",
        "  while not policy_stable:\n",
        "    V = policy_evaluation(P, R, gamma, policy, theta)\n",
        "    policy_stable = policy_improvement(P, R, gamma, policy, V)\n",
        "\n",
        "  return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn0HFryxUd45"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-7s2t_ttobMJ"
      },
      "outputs": [],
      "source": [
        "def is_same_optimal_value(V1, V2, diff_theta=1e-3):\n",
        "  diff = np.abs(V1 - V2)\n",
        "  return np.all(diff < diff_theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DFP678PtlhBI"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mokqMJvA7tPW"
      },
      "outputs": [],
      "source": [
        "env = Environment(GRID_WORLD)\n",
        "P, R = env.get_transition_prob_and_expected_reward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBOiuj8nlYNB"
      },
      "source": [
        "#### Exercise 1: Using Policy iteration algorithm find the optimal path from start to goal position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT6iIL3KPxGM",
        "outputId": "616a6912-e828-4277-b54f-5c71ee0fc414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
          ]
        }
      ],
      "source": [
        "# # Start with random choice of init_policy.\n",
        "# One such choice could be: init_policy = np.ones(env.num_states, dtype=int)\n",
        "init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))\n",
        "\n",
        "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
        "pitr_path = env.find_path(pitr_policy)\n",
        "print(pitr_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPAR8sVfpg5x"
      },
      "source": [
        "#### Exercise 2: Using initial guess for V as random values, find the optimal value function using policy evaluation and compare it with the optimal value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKhx-N2ipe6M",
        "outputId": "9885a08e-d47a-4839-98b9-d6f9ba46d41e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Using the initialisation as given in the code: init_V = 10*np.ones(env.num_states)\n",
        "\n",
        "init_V = 10*np.ones(env.num_states)\n",
        "\n",
        "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
        "is_same_optimal_value(pitr_V_star, V_star)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using completely random initialisation: init_V = np.random.randn(env.num_states)\n",
        "\n",
        "init_V = np.random.randn(env.num_states)\n",
        "\n",
        "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
        "is_same_optimal_value(pitr_V_star, V_star)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vULEobUn9Ybd",
        "outputId": "7d7231b6-6ce6-4931-c216-7a818ce9ef27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using He initialisation: init_V = np.random.randn(env.num_states)*np.sqrt(2/(env.num_states-1))\n",
        "\n",
        "init_V = np.random.randn(env.num_states)*np.sqrt(2/(env.num_states-1))\n",
        "\n",
        "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
        "is_same_optimal_value(pitr_V_star, V_star)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7ZbSfQW9iS2",
        "outputId": "fe91d142-14f5-4a50-be5d-a691f621f5fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Xavier initialisation: init_V = np.random.randn(env.num_states)*np.sqrt(2/(2*env.num_states))\n",
        "\n",
        "init_V = np.random.randn(env.num_states)*np.sqrt(2/(2*env.num_states))\n",
        "\n",
        "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
        "is_same_optimal_value(pitr_V_star, V_star)\n",
        "\n",
        "## Regardless of initialisation condition there is no change in policy calculation. It is always able to find the optimal path."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV_1QIg9_csG",
        "outputId": "44fd1b6b-cab1-4afb-a2ed-126f46b5faee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1wMz4kGJBsP"
      },
      "source": [
        "#### To-do: Repeat Exercise 1 with a random Deterministic policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using np.ones, here a random choice was used for initialising actions. Even then, the policy converges to the optimal one\n",
        "\n",
        "init_policy = DeterministicPolicy(actions=np.random.choice(len(Actions),env.num_states))\n",
        "\n",
        "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
        "pitr_path = env.find_path(pitr_policy)\n",
        "print(pitr_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIZeFuKQ_wsj",
        "outputId": "c7b090a4-b97e-4b60-a51c-e20ce9943649"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_4I7x4rMlFgp"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}