{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OBTUWvpGP5HT"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "\n",
        "  def __init__(self,state_size=4,hidden_size=128,num_actions=2):\n",
        "    super(Policy,self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(state_size,hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size,num_actions)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    fcHid = F.relu(self.fc1(x))\n",
        "    out = self.fc2(fcHid)\n",
        "    p = F.softmax(out,dim=1)\n",
        "\n",
        "    return p\n",
        "\n",
        "  def chooseAct(self,x):\n",
        "    x = torch.from_numpy(x).float().unsqueeze(0)\n",
        "    prob = self.forward(x)\n",
        "    model = Categorical(prob)\n",
        "    act = model.sample() # randomly sample an action\n",
        "\n",
        "    return act.item(),model.log_prob(act)\n"
      ],
      "metadata": {
        "id": "fxA2G8LLq-H-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer, num_episodes = 500, max_timesteps = 500, gamma = 0.99):\n",
        "  scores = []\n",
        "  scores_deque = deque(maxlen = 100)\n",
        "  for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    log_probs_all = []\n",
        "    # simulate an episode until end\n",
        "    for t in range(max_timesteps):\n",
        "      action,log_probs = policy.chooseAct(state)\n",
        "      state,reward,done,_ = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      log_probs_all.append(log_probs)\n",
        "      if done:\n",
        "        break\n",
        "    scores.append(sum(rewards))\n",
        "    scores_deque.append(scores)\n",
        "\n",
        "    # calculate discounted returns for the episode using rewards obtained\n",
        "    discounted_returns = []\n",
        "    sumR = 0\n",
        "    for r in reversed(rewards):\n",
        "      sumR = sumR + gamma * r #r + gamma*(sumR)\n",
        "      discounted_returns.insert(0,sumR)\n",
        "\n",
        "    # Calculate the loss\n",
        "    policy_loss = []\n",
        "    i = 0\n",
        "    for log_prob in log_probs_all:\n",
        "        R = discounted_returns[i]\n",
        "        policy_loss.append(-log_prob * R)\n",
        "        i += 1\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Episode {episode}: Reward = {sum(rewards)}\")\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "eqmwsIMsvZ2K"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 1000\n",
        "num_runs = 5\n",
        "scores_all_runs = np.zeros((num_runs,n_episodes))\n",
        "for r in range(num_runs):\n",
        "  env = gym.make('CartPole-v1')\n",
        "  policy = Policy()\n",
        "  optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "  scores = reinforce(policy, optimizer, num_episodes = n_episodes)\n",
        "  scores_all_runs[r,:] = scores"
      ],
      "metadata": {
        "id": "hj0JLpDEw9XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the scores file across 5 runs\n",
        "import pandas as pd\n",
        "file_path = '/content/REINFORCE_wout_baseline_cartpole.xlsx'\n",
        "\n",
        "pd.DataFrame(scores_all_runs).to_excel(file_path,index=False)\n"
      ],
      "metadata": {
        "id": "YF9j9HwF-WjP"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}